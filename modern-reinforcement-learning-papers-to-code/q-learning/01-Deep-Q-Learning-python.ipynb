{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0065612c-f8db-4cb0-8965-d9bec32e8fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gymnasium\n",
      "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.23.5)\n",
      "Collecting jax-jumpy>=1.0.0 (from gymnasium)\n",
      "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.5.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m286.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m253.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m236.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m180.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m268.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m262.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Collecting cmake (from triton==2.0.0->torch)\n",
      "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m221.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch)\n",
      "  Downloading lit-16.0.1.tar.gz (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m273.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium)\n",
      "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium)\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (8.1.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (4.65.0)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium)\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m288.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium)\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium) (5.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2022.12.7)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license, lit\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=20fde006fcefef63c6bf8e934a3fdeba5f692309d6cb0ff7543ef254ac57b999\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dok3_uc1/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.1-py3-none-any.whl size=88172 sha256=814b22417b344187964943aa9483230975cd4b72230f42bf39bd5299cd976762\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dok3_uc1/wheels/33/c2/b7/b91592eb5167b4293827b97fb02d52686773dded13f7fb1054\n",
      "Successfully built AutoROM.accept-rom-license lit\n",
      "Installing collected packages: lit, farama-notifications, cmake, opencv-python, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jax-jumpy, filelock, ale-py, nvidia-cusolver-cu11, nvidia-cudnn-cu11, gymnasium, AutoROM.accept-rom-license, autorom, shimmy, triton, torch\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 cmake-3.26.3 farama-notifications-0.0.4 filelock-3.12.0 gymnasium-0.28.1 jax-jumpy-1.0.0 lit-16.0.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 opencv-python-4.7.0.72 shimmy-0.2.1 torch-2.0.0 triton-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium torch opencv-python gymnasium[atari] gymnasium[accept-rom-license] matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f55985-d02c-48a8-b8f9-fd203e164f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.spaces\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46d222b-ae2b-4e9b-aaf9-6dd5a2f30d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RepeatActionMaxFrame(gym.Wrapper):\n",
    "    '''\n",
    "        Atari Preprocessing\n",
    "        \n",
    "        - Used to solve the issue where the atari library only renders some images on even frames and others on odd frames.\n",
    "        - Handles the ability to repeat the last action a number of times to lighten the load of processing every frame. \n",
    "    '''\n",
    "    def __init__(self, environment, repeat = 4, clip_reward = False, no_ops = 0, fire_first = False):\n",
    "        super(RepeatActionMaxFrame, self).__init__(environment)\n",
    "        self.environment = environment\n",
    "        self.shape = environment.observation_space.low.shape\n",
    "        self.repeat = repeat\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_first = fire_first\n",
    "        \n",
    "        #init the frame buffer to hold our 2 frames (overcome flickering)\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, terminated, truncated, info = self.environment.step(action)\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
    "            total_reward += reward\n",
    "        \n",
    "            index = i % 2\n",
    "            self.frame_buffer[index] = observation\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
    "        return max_frame, total_reward, terminated, truncated, info    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        observation, properties = self.environment.reset()\n",
    "        no_ops = np.random.randint(self.no_ops) + 1 if self.no_ops > 0 else 0\n",
    "        for _ in range(no_ops):\n",
    "            _, _, terminated, truncated, _ = self.environment.step(0)\n",
    "            if done:\n",
    "                self.environment.reset()\n",
    "        \n",
    "        if self.fire_first:\n",
    "            assert self.environment.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            observation, _, _, _, _ = self.environment.step(0)\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        self.frame_buffer[0] = observation \n",
    "        return observation, properties\n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc482f8a-386c-4c48-81cc-772f4b687aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    '''\n",
    "        Resize the frame\n",
    "    '''\n",
    "    def __init__(self, environment, new_shape):\n",
    "        super(PreprocessFrame, self).__init__(environment)\n",
    "        self.environment = environment\n",
    "        self.shape = (new_shape[2], new_shape[0], new_shape[1]) \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
    "        \n",
    "    def observation(self, raw_observation):\n",
    "        im = cv2.cvtColor(raw_observation, cv2.COLOR_RGB2GRAY)\n",
    "        im = cv2.resize(im, self.shape[1:], interpolation = cv2.INTER_AREA)\n",
    "        im = np.array(im, dtype = np.uint8).reshape(self.shape)\n",
    "        \n",
    "        im = im / 255.0\n",
    "        \n",
    "        return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34de29b0-bd05-4a63-9b8e-2fd8b4101988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackFrames(gym.ObservationWrapper):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, environment, stack_size):\n",
    "        super(StackFrames, self).__init__(environment)\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.observation_space = gym.spaces.Box(environment.observation_space.low.repeat(stack_size, axis = 1), \n",
    "                                                environment.observation_space.high.repeat(stack_size, axis = 1), \n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "        self.frame_stack = deque(maxlen = stack_size)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.frame_stack.clear()\n",
    "        \n",
    "        observation, properties = self.environment.reset()\n",
    "        \n",
    "        for _ in range(self.frame_stack.maxlen):\n",
    "            self.frame_stack.append(observation)\n",
    "\n",
    "        return np.array(self.frame_stack).reshape(self.observation_space.low.shape), properties\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.frame_stack.append(observation)\n",
    "        return np.array(self.frame_stack).reshape(self.observation_space.low.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9251e140-636e-486a-9c13-434a51d71338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_environment(environment_name, shape = (84, 84, 1), repeat = 4, clip_reward = False, no_ops = 0, fire_first = False):\n",
    "    environment = gym.make(environment_name)\n",
    "    environment = RepeatActionMaxFrame(environment, repeat, clip_reward, no_ops, fire_first)\n",
    "    environment = PreprocessFrame(environment, shape)\n",
    "    environment = StackFrames(environment, repeat)\n",
    "    \n",
    "    return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c68abc-abed-453b-a4b8-05b7c7b62ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, epsilons, filename):\n",
    "    figure = plt.figure()\n",
    "    ax = figure.add_subplot(111, label = \"1\")\n",
    "    ax2 = figure.add_subplot(111, label = \"2\", frame_on = False)\n",
    "    \n",
    "    ax.plot(x, epsilons, color = \"C0\")\n",
    "    ax.set_xlabel(\"Training Steps\", color = \"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color = \"C0\")\n",
    "    ax.tick_params(axis = \"x\", colors = \"C0\")\n",
    "    ax.tick_params(axis = \"y\", colors = \"C0\")\n",
    "    \n",
    "    N = len(scores)\n",
    "    running_average = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_average[t] = np.mean(scores[max(0, t - 100):(t + 1)])\n",
    "        \n",
    "    ax2.scatter(x, running_average, color = \"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel(\"Score\", color = \"C1\")\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.tick_params(axis = \"y\", colors = \"C1\")\n",
    "    \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce40121-986a-4421-8e85-33536a582ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, num_classes, file_name, input_dimensions, checkpoint_directory):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_directory, file_name)\n",
    "        \n",
    "        self.conv_layer_1 = nn.Conv2d(input_dimensions[0], 32, 8, stride=4, device = self.device)\n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, 4, stride=2, device = self.device)\n",
    "        self.conv_layer_3 = nn.Conv2d(64, 32, 3, stride=1, device = self.device)\n",
    "        \n",
    "        fc_input_size = self.find_input_size(input_dimensions)\n",
    "        \n",
    "        self.fully_connected_1 = nn.Linear(fc_input_size, 512, device = self.device)\n",
    "        self.fully_connected_2 = nn.Linear(512, num_classes, device = self.device)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr = learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        con_layer1_output = F.relu(self.conv_layer_1(data))\n",
    "        con_layer2_output = F.relu(self.conv_layer_2(con_layer1_output))\n",
    "        con_layer3_output = F.relu(self.conv_layer_3(con_layer2_output))\n",
    "        \n",
    "        conv_state = con_layer3_output.view(con_layer3_output.size()[0], -1) # flatten the convolutions\n",
    "        \n",
    "        fc_layer1_output = F.relu(self.fully_connected_1(conv_state))\n",
    "        fc_layer2_output = self.fully_connected_2(fc_layer1_output)\n",
    "        \n",
    "        return fc_layer2_output\n",
    "    \n",
    "    def learn(self, data, labels):\n",
    "        self.optimizer.zero_grad()\n",
    "        data = T.tensor(data).to(self.device)\n",
    "        labels = T.tensor(labels).to(self.device)\n",
    "        \n",
    "        predictions = self.forward(data)\n",
    "        \n",
    "        cost = self.loss(predictions, labels)\n",
    "        \n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def find_input_size(self, input_dimensions):\n",
    "        input_zeros = T.zeros(1, *input_dimensions).to(self.device)\n",
    "        dims = self.conv_layer_1(input_zeros)\n",
    "        dims = self.conv_layer_2(dims)\n",
    "        dims = self.conv_layer_3(dims)\n",
    "        #print(int(np.prod(dims.size())))\n",
    "        return int(np.prod(dims.size()))\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print('... Saving Checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        print('... Loading Checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d1087c-523f-4cb3-b092-7d26d59c951c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, num_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype = np.float32)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.mem_size, dtype = np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        \n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype = np.uint8)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, states_, dones\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dfd16e-37e2-4be3-a3ba-e6b5873bcd9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepQAgent():\n",
    "    def __init__(self, learning_rate = 0.0001, gamma = 0.99, epsilon = 1.0, epsilon_min = 0.01, epsilon_decrement = 5e-7, \\\n",
    "                 num_actions = 2, memory_size = 2, batch_size = 32, num_states = (4, 1), algorithm = None, replace = 1000, input_dimensions = 8, \\\n",
    "                 checkpoint_directory = 'checkpoints/dqn', environment_name = None):\n",
    "        self.learning_rate = learning_rate # alpha\n",
    "        self.gamma = gamma # discount_factor\n",
    "        self.epsilon = epsilon # explore exploit rate\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decrement = epsilon_decrement\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.algorithm = algorithm\n",
    "        self.replace_target_count = replace\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.environment_name = environment_name\n",
    "        self.action_space = [i for i in range(self.num_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        self.memory = ReplayBuffer(memory_size, input_dimensions, num_actions)\n",
    "        \n",
    "        self.q_eval = DeepQNetwork(self.learning_rate, self.num_actions, input_dimensions = self.input_dimensions, file_name = self.environment_name + '_' + self.algorithm + '_q_eval', \\\n",
    "                                   checkpoint_directory = self.checkpoint_directory)\n",
    "        \n",
    "        self.q_next = DeepQNetwork(self.learning_rate, self.num_actions, input_dimensions = self.input_dimensions, file_name = self.environment_name + '_' + self.algorithm + '_q_next', \\\n",
    "                                   checkpoint_directory = self.checkpoint_directory)\n",
    "        \n",
    "        \n",
    "    def choose_action(self, current_state):\n",
    "        random_number = np.random.random()\n",
    "        if random_number < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            return T.argmax(self.q_eval.forward(T.tensor([current_state], dtype=T.float).to(self.q_eval.device))).item()\n",
    "            \n",
    "    def store_transition(self, state, action, reward, state_, terminated):\n",
    "        self.memory.store_transition(state, action, reward, state_, terminated)\n",
    "        \n",
    "    def sample_memory(self):\n",
    "        state, action, reward, state_, terminated = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        terminateds = T.tensor(terminated).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(state_).to(self.q_eval.device)\n",
    "        \n",
    "        return states, actions, rewards, states_, terminateds\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_count == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            \n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        Decrement epsilon\n",
    "        '''\n",
    "        self.epsilon = self.epsilon - self.epsilon_decrement if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "        \n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        self.replace_target_network()\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        \n",
    "        indices = np.arange(self.batch_size)\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_).max(dim = 1)[0]\n",
    "        \n",
    "        q_next[dones == 1] = 0.0 # Masking by the dones\n",
    "        q_target = rewards + self.gamma * q_next\n",
    "        \n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        self.decrement_epsilon()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e71be-6bb3-4f0f-96e9-62d572cfc142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/tmp/ipykernel_101/3598166615.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return T.argmax(self.q_eval.forward(T.tensor([current_state], dtype=T.float).to(self.q_eval.device))).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: -21.0 Average Score: -2e+01 Best Score: -inf Epsilon: 0.99 Steps: 824\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 10 Score: -21.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.91 Steps: 9216\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 20 Score: -21.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.82 Steps: 18462\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 30 Score: -19.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.71 Steps: 28743\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "env_name = 'PongNoFrameskip-v4'\n",
    "environment = make_environment(env_name)\n",
    "best_score = -np.inf\n",
    "load_checkpoint = False\n",
    "number_games = 300\n",
    "\n",
    "agent = DeepQAgent(gamma = 0.99, epsilon = 1.0, learning_rate = 0.0001, input_dimensions = (environment.observation_space.shape), num_actions = environment.action_space.n, \\\n",
    "                   memory_size = 80000, epsilon_min = 0.1, batch_size = 32, replace = 1000, epsilon_decrement = 1e-5, checkpoint_directory = 'models/', algorithm = 'DQNAgent', \\\n",
    "                   environment_name = env_name)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "filename = agent.algorithm + '_' + agent.environment_name + '_lr' + str(agent.learning_rate) + '_' + \\\n",
    "    str(number_games) + '_games'\n",
    "figure_file = 'plots/' + filename + '.png'\n",
    "\n",
    "num_steps = 0\n",
    "scores, epsilon_history, steps_array = [], [], []\n",
    "\n",
    "for game_counter in range(number_games):\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    observation, properties = environment.reset()\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, terminated, truncated, info = environment.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation, action, reward, observation_, int(terminated))\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "        num_steps += 1\n",
    "        \n",
    "    scores.append(score)\n",
    "    steps_array.append(num_steps)\n",
    "    \n",
    "    average_score = np.mean(scores[-100:])\n",
    "    if game_counter % 10 == 0:\n",
    "        print(f\"Episode: {game_counter} Score: {score} Average Score: {average_score:.1} Best Score: {best_score:.1} Epsilon: {agent.epsilon:.2} Steps: {num_steps}\")\n",
    "    \n",
    "    if average_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score = average_score\n",
    "    \n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2646f89-fdc9-463a-86ea-0494dcc4e88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(steps_array, scores, epsilon_history, figure_file)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
