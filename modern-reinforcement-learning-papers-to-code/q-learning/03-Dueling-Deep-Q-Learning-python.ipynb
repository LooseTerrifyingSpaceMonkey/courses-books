{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0065612c-f8db-4cb0-8965-d9bec32e8fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.9/dist-packages (0.28.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.6.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (4.4.0)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (6.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.23.4)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (0.2.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (0.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (2.28.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (8.1.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.9/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium) (5.10.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium torch opencv-python gymnasium[atari] gymnasium[accept-rom-license] matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f55985-d02c-48a8-b8f9-fd203e164f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.spaces\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46d222b-ae2b-4e9b-aaf9-6dd5a2f30d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RepeatActionMaxFrame(gym.Wrapper):\n",
    "    '''\n",
    "        Atari Preprocessing\n",
    "        \n",
    "        - Used to solve the issue where the atari library only renders some images on even frames and others on odd frames.\n",
    "        - Handles the ability to repeat the last action a number of times to lighten the load of processing every frame. \n",
    "    '''\n",
    "    def __init__(self, environment, repeat = 4, clip_reward = False, no_ops = 0, fire_first = False):\n",
    "        super(RepeatActionMaxFrame, self).__init__(environment)\n",
    "        self.environment = environment\n",
    "        self.shape = environment.observation_space.low.shape\n",
    "        self.repeat = repeat\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_first = fire_first\n",
    "        \n",
    "        #init the frame buffer to hold our 2 frames (overcome flickering)\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        for i in range(self.repeat):\n",
    "            observation, reward, terminated, truncated, info = self.environment.step(action)\n",
    "            if self.clip_reward:\n",
    "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
    "            total_reward += reward\n",
    "        \n",
    "            index = i % 2\n",
    "            self.frame_buffer[index] = observation\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            \n",
    "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
    "        return max_frame, total_reward, terminated, truncated, info    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        observation, properties = self.environment.reset()\n",
    "        no_ops = np.random.randint(self.no_ops) + 1 if self.no_ops > 0 else 0\n",
    "        for _ in range(no_ops):\n",
    "            _, _, terminated, truncated, _ = self.environment.step(0)\n",
    "            if done:\n",
    "                self.environment.reset()\n",
    "        \n",
    "        if self.fire_first:\n",
    "            assert self.environment.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "            observation, _, _, _, _ = self.environment.step(0)\n",
    "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
    "        self.frame_buffer[0] = observation \n",
    "        return observation, properties\n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc482f8a-386c-4c48-81cc-772f4b687aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessFrame(gym.ObservationWrapper):\n",
    "    '''\n",
    "        Resize the frame\n",
    "    '''\n",
    "    def __init__(self, environment, new_shape):\n",
    "        super(PreprocessFrame, self).__init__(environment)\n",
    "        self.environment = environment\n",
    "        self.shape = (new_shape[2], new_shape[0], new_shape[1]) \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, high = 1.0, shape = self.shape, dtype = np.float32)\n",
    "        \n",
    "    def observation(self, raw_observation):\n",
    "        im = cv2.cvtColor(raw_observation, cv2.COLOR_RGB2GRAY)\n",
    "        im = cv2.resize(im, self.shape[1:], interpolation = cv2.INTER_AREA)\n",
    "        im = np.array(im, dtype = np.uint8).reshape(self.shape)\n",
    "        \n",
    "        im = im / 255.0\n",
    "        \n",
    "        return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34de29b0-bd05-4a63-9b8e-2fd8b4101988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StackFrames(gym.ObservationWrapper):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, environment, stack_size):\n",
    "        super(StackFrames, self).__init__(environment)\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.observation_space = gym.spaces.Box(environment.observation_space.low.repeat(stack_size, axis = 1), \n",
    "                                                environment.observation_space.high.repeat(stack_size, axis = 1), \n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "        self.frame_stack = deque(maxlen = stack_size)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.frame_stack.clear()\n",
    "        \n",
    "        observation, properties = self.environment.reset()\n",
    "        \n",
    "        for _ in range(self.frame_stack.maxlen):\n",
    "            self.frame_stack.append(observation)\n",
    "\n",
    "        return np.array(self.frame_stack).reshape(self.observation_space.low.shape), properties\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        self.frame_stack.append(observation)\n",
    "        return np.array(self.frame_stack).reshape(self.observation_space.low.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9251e140-636e-486a-9c13-434a51d71338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_environment(environment_name, shape = (84, 84, 1), repeat = 4, clip_reward = False, no_ops = 0, fire_first = False):\n",
    "    environment = gym.make(environment_name)\n",
    "    environment = RepeatActionMaxFrame(environment, repeat, clip_reward, no_ops, fire_first)\n",
    "    environment = PreprocessFrame(environment, shape)\n",
    "    environment = StackFrames(environment, repeat)\n",
    "    \n",
    "    return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afbe2a1-25a7-4e19-9321-37509e4c5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, epsilons, filename):\n",
    "    figure = plt.figure()\n",
    "    ax = figure.add_subplot(111, label = \"1\")\n",
    "    ax2 = figure.add_subplot(111, label = \"2\", frame_on = False)\n",
    "    \n",
    "    ax.plot(x, epsilons, color = \"C0\")\n",
    "    ax.set_xlabel(\"Training Steps\", color = \"C0\")\n",
    "    ax.set_ylabel(\"Epsilon\", color = \"C0\")\n",
    "    ax.tick_params(axis = \"x\", colors = \"C0\")\n",
    "    ax.tick_params(axis = \"y\", colors = \"C0\")\n",
    "    \n",
    "    N = len(scores)\n",
    "    running_average = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_average[t] = np.mean(scores[max(0, t - 100):(t + 1)])\n",
    "        \n",
    "    ax2.scatter(x, running_average, color = \"C1\")\n",
    "    ax2.axes.get_xaxis().set_visible(False)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.set_ylabel(\"Score\", color = \"C1\")\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.tick_params(axis = \"y\", colors = \"C1\")\n",
    "    \n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d1087c-523f-4cb3-b092-7d26d59c951c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, num_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape), dtype = np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape), dtype = np.float32)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.mem_size, dtype = np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype = np.float32)\n",
    "        \n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype = np.uint8)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace = False)\n",
    "        \n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        \n",
    "        return states, actions, rewards, states_, dones\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce40121-986a-4421-8e85-33536a582ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DuelingDeepQNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, num_classes, file_name, input_dimensions, checkpoint_directory):\n",
    "        super(DuelingDeepQNetwork, self).__init__()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_directory, file_name)\n",
    "        \n",
    "        self.conv_layer_1 = nn.Conv2d(input_dimensions[0], 32, 8, stride=4, device = self.device)\n",
    "        self.conv_layer_2 = nn.Conv2d(32, 64, 4, stride=2, device = self.device)\n",
    "        self.conv_layer_3 = nn.Conv2d(64, 32, 3, stride=1, device = self.device)\n",
    "        \n",
    "        fc_input_size = self.find_input_size(input_dimensions)\n",
    "        \n",
    "        self.fully_connected_1 = nn.Linear(fc_input_size, 512, device = self.device)\n",
    "\n",
    "        self.fully_connected_value = nn.Linear(512, 1, device = self.device)\n",
    "        self.fully_connected_advantage = nn.Linear(512, num_classes, device = self.device)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr = learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        con_layer1_output = F.relu(self.conv_layer_1(data))\n",
    "        con_layer2_output = F.relu(self.conv_layer_2(con_layer1_output))\n",
    "        con_layer3_output = F.relu(self.conv_layer_3(con_layer2_output))\n",
    "        \n",
    "        conv_state = con_layer3_output.view(con_layer3_output.size()[0], -1) # flatten the convolutions\n",
    "        \n",
    "        fc_layer1_output = F.relu(self.fully_connected_1(conv_state))\n",
    "        \n",
    "        fc_value_output = self.fully_connected_value(fc_layer1_output)\n",
    "        \n",
    "        fc_advantage_output = self.fully_connected_advantage(fc_layer1_output)\n",
    "      \n",
    "        return fc_value_output, fc_advantage_output\n",
    "    \n",
    "    def learn(self, data, labels):\n",
    "        self.optimizer.zero_grad()\n",
    "        data = T.tensor(data).to(self.device)\n",
    "        labels = T.tensor(labels).to(self.device)\n",
    "        \n",
    "        predictions = self.forward(data)\n",
    "        \n",
    "        cost = self.loss(predictions, labels)\n",
    "        \n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def find_input_size(self, input_dimensions):\n",
    "        input_zeros = T.zeros(1, *input_dimensions).to(self.device)\n",
    "        dims = self.conv_layer_1(input_zeros)\n",
    "        dims = self.conv_layer_2(dims)\n",
    "        dims = self.conv_layer_3(dims)\n",
    "        #print(int(np.prod(dims.size())))\n",
    "        return int(np.prod(dims.size()))\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print('... Saving Checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        print('... Loading Checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dfd16e-37e2-4be3-a3ba-e6b5873bcd9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DuelingDeepQAgent():\n",
    "    def __init__(self, learning_rate = 0.0001, gamma = 0.99, epsilon = 1.0, epsilon_min = 0.01, epsilon_decrement = 5e-7, \\\n",
    "                 num_actions = 2, memory_size = 2, batch_size = 32, num_states = (4, 1), algorithm = None, replace = 1000, input_dimensions = 8, \\\n",
    "                 checkpoint_directory = 'checkpoints/dqn', environment_name = None):\n",
    "        self.learning_rate = learning_rate # alpha\n",
    "        self.gamma = gamma # discount_factor\n",
    "        self.epsilon = epsilon # explore exploit rate\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decrement = epsilon_decrement\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.algorithm = algorithm\n",
    "        self.replace_target_count = replace\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.environment_name = environment_name\n",
    "        self.action_space = [i for i in range(self.num_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        \n",
    "        self.memory = ReplayBuffer(memory_size, input_dimensions, num_actions)\n",
    "        \n",
    "        self.q_eval = DuelingDeepQNetwork(self.learning_rate, self.num_actions, input_dimensions = self.input_dimensions, file_name = self.environment_name + '_' + self.algorithm + '_q_eval', \\\n",
    "                                   checkpoint_directory = self.checkpoint_directory)\n",
    "        \n",
    "        self.q_next = DuelingDeepQNetwork(self.learning_rate, self.num_actions, input_dimensions = self.input_dimensions, file_name = self.environment_name + '_' + self.algorithm + '_q_next', \\\n",
    "                                   checkpoint_directory = self.checkpoint_directory)\n",
    "        \n",
    "        \n",
    "    def choose_action(self, current_state):\n",
    "        random_number = np.random.random()\n",
    "        if random_number < self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        else:\n",
    "            _, advantage = self.q_eval.forward(T.tensor([current_state], dtype=T.float).to(self.q_eval.device))\n",
    "            return T.argmax(advantage).item()\n",
    "            \n",
    "    def store_transition(self, state, action, reward, state_, terminated):\n",
    "        self.memory.store_transition(state, action, reward, state_, terminated)\n",
    "        \n",
    "    def sample_memory(self):\n",
    "        state, action, reward, state_, terminated = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        terminateds = T.tensor(terminated).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(state_).to(self.q_eval.device)\n",
    "        \n",
    "        return states, actions, rewards, states_, terminateds\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_count == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "            \n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        Decrement epsilon\n",
    "        '''\n",
    "        self.epsilon = self.epsilon - self.epsilon_decrement if self.epsilon > self.epsilon_min else self.epsilon_min\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "        \n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.memory.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        self.replace_target_network()\n",
    "        \n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        \n",
    "        indices = np.arange(self.batch_size)\n",
    "        \n",
    "        V_s, A_s = self.q_eval.forward(states)\n",
    "        V_s_, A_s_ = self.q_next.forward(states_)\n",
    "        \n",
    "        q_pred = T.add(V_s, (A_s - A_s.mean(dim = 1, keepdim = True)))[indices, actions]\n",
    "        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim = 1, keepdim = True))).max(dim = 1)[0]\n",
    "        \n",
    "        q_next[dones == 1] = 0.0\n",
    "        q_target = rewards + self.gamma * q_next\n",
    "        \n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        \n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        self.decrement_epsilon()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1686a83-14d2-4e71-a41f-df1d7b25d1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/tmp/ipykernel_194/2405011126.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  _, advantage = self.q_eval.forward(T.tensor([current_state], dtype=T.float).to(self.q_eval.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: -20.0 Average Score: -2e+01 Best Score: -inf Epsilon: 0.99 Steps: 925\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 10 Score: -21.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.9 Steps: 10327\n",
      "Episode: 20 Score: -17.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.79 Steps: 20637\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 30 Score: -21.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.69 Steps: 31327\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 40 Score: -18.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.55 Steps: 44588\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 50 Score: -19.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.42 Steps: 58479\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 60 Score: -18.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.23 Steps: 77154\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 70 Score: -15.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.1 Steps: 100074\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 80 Score: -5.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.1 Steps: 125882\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 90 Score: -9.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.1 Steps: 152461\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 100 Score: 4.0 Average Score: -2e+01 Best Score: -2e+01 Epsilon: 0.1 Steps: 180249\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 110 Score: -5.0 Average Score: -1e+01 Best Score: -1e+01 Epsilon: 0.1 Steps: 212366\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 120 Score: 9.0 Average Score: -1e+01 Best Score: -1e+01 Epsilon: 0.1 Steps: 242668\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 130 Score: 15.0 Average Score: -8e+00 Best Score: -8e+00 Epsilon: 0.1 Steps: 267025\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 140 Score: 13.0 Average Score: -5e+00 Best Score: -5e+00 Epsilon: 0.1 Steps: 289767\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 150 Score: 19.0 Average Score: -1e+00 Best Score: -2e+00 Epsilon: 0.1 Steps: 311153\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 160 Score: 16.0 Average Score: 2e+00 Best Score: 1e+00 Epsilon: 0.1 Steps: 334651\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 170 Score: 16.0 Average Score: 4e+00 Best Score: 4e+00 Epsilon: 0.1 Steps: 356720\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 180 Score: 18.0 Average Score: 7e+00 Best Score: 6e+00 Epsilon: 0.1 Steps: 379937\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 190 Score: 17.0 Average Score: 9e+00 Best Score: 9e+00 Epsilon: 0.1 Steps: 400850\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "Episode: 200 Score: 14.0 Average Score: 1e+01 Best Score: 1e+01 Epsilon: 0.1 Steps: 423351\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n",
      "... Saving Checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "env_name = 'PongNoFrameskip-v4'\n",
    "environment = make_environment(env_name)\n",
    "best_score = -np.inf\n",
    "load_checkpoint = False\n",
    "number_games = 300\n",
    "\n",
    "agent = DuelingDeepQAgent(gamma = 0.99, epsilon = 1.0, learning_rate = 0.0001, input_dimensions = (environment.observation_space.shape), num_actions = environment.action_space.n, \\\n",
    "                   memory_size = 80000, epsilon_min = 0.1, batch_size = 32, replace = 1000, epsilon_decrement = 1e-5, checkpoint_directory = 'models/', algorithm = 'DuelingDQNAgent', \\\n",
    "                   environment_name = env_name)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "filename = agent.algorithm + '_' + agent.environment_name + '_lr' + str(agent.learning_rate) + '_' + \\\n",
    "    str(number_games) + '_games'\n",
    "figure_file = 'plots/' + filename + '.png'\n",
    "\n",
    "num_steps = 0\n",
    "scores, epsilon_history, steps_array = [], [], []\n",
    "\n",
    "for game_counter in range(number_games):\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    observation, properties = environment.reset()\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, terminated, truncated, info = environment.step(action)\n",
    "        score += reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation, action, reward, observation_, int(terminated))\n",
    "            agent.learn()\n",
    "        observation = observation_\n",
    "        num_steps += 1\n",
    "        \n",
    "    scores.append(score)\n",
    "    steps_array.append(num_steps)\n",
    "    \n",
    "    average_score = np.mean(scores[-100:])\n",
    "    if game_counter % 10 == 0:\n",
    "        print(f\"Episode: {game_counter} Score: {score} Average Score: {average_score:.1} Best Score: {best_score:.1} Epsilon: {agent.epsilon:.2} Steps: {num_steps}\")\n",
    "    \n",
    "    if average_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score = average_score\n",
    "    \n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf7789-1fc7-4a4b-be5c-60b3279326ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(steps_array, scores, epsilon_history, figure_file)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
